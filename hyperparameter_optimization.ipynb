{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna code for ridge model\n",
    "\n",
    "def objective_ridge(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 0.001, 100.0, log=True),\n",
    "        'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga']),\n",
    "        'random_state': 42,\n",
    "        'tol': 1e-4 \n",
    "    }\n",
    "    \n",
    "    model = Ridge(**param)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=300)\n",
    "    \n",
    "    score = cross_val_score(model, X_train_preprocessed, y_train, \n",
    "                            cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    return np.sqrt(-score.mean())\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study_ridge = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_ridge.optimize(objective_ridge, n_trials=30)\n",
    "\n",
    "final_ridge = Ridge(**study_ridge.best_params, random_state=30)\n",
    "\n",
    "final_ridge.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred_ridge = final_ridge.predict(X_test_preprocessed)\n",
    "actual_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "\n",
    "print(f\"Best Ridge RMSE: {study_ridge.best_value}\")\n",
    "print(f\"Internal Test RMSE: {actual_test_rmse}\")\n",
    "print(f\"Best Ridge params: {study_ridge.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna code for XGBRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0), \n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0), \n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**param)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    score = cross_val_score(model, X_train_preprocessed, y_train,  \n",
    "                            cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    return -score.mean() \n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=50) \n",
    "\n",
    "final_xgb = XGBRegressor(**study.best_params, random_state=35)\n",
    "final_xgb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred_xgb = final_xgb.predict(X_test_preprocessed)\n",
    "actual_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "\n",
    "\n",
    "print(f\"Best XGB RMSE: {study.best_value}\")\n",
    "print(f\"Internal Test RMSE: {actual_test_rmse}\")\n",
    "print(f\"Best XGB params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna code for GradientBoostingRegressor\n",
    "\n",
    "def objective_gbr(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 4000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 0.8),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 0.5),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 25),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = GradientBoostingRegressor(**param)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    score = cross_val_score(model, X_train_preprocessed, y_train, \n",
    "                            cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    return -score.mean()\n",
    "\n",
    "sampler_gbr = optuna.samplers.TPESampler(seed=42)\n",
    "study_gbr = optuna.create_study(direction='minimize', sampler=sampler_gbr)\n",
    "study_gbr.optimize(objective_gbr, n_trials=50)\n",
    "\n",
    "final_gbr = GradientBoostingRegressor(**study_gbr.best_params, random_state=400)\n",
    "final_gbr.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred_gbr = final_gbr.predict(X_test_preprocessed)\n",
    "actual_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "\n",
    "print(f\"Best GBR RMSE: {study_gbr.best_value}\")\n",
    "print(f\"Internal Test RMSE: {actual_test_rmse}\")\n",
    "print(f\"Best GBR Params: {study_gbr.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna code for LGBMRegressor\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 45, \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2000, 4000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 7),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=45) \n",
    "    \n",
    "    score = cross_val_score(model, X_train_preprocessed, y_train, \n",
    "                            cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    return -score.mean()\n",
    "\n",
    "sampler_lgbm = optuna.samplers.TPESampler(seed=45) \n",
    "study_lgbm = optuna.create_study(direction='minimize', sampler=sampler_lgbm)\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=50)\n",
    "\n",
    "final_lgbm = lgb.LGBMRegressor(**study_lgbm.best_params, random_state=400, verbose=-1)\n",
    "final_lgbm.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred_lgbm = final_lgbm.predict(X_test_preprocessed)\n",
    "actual_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\n",
    "\n",
    "\n",
    "print(f\"Best LGBM RMSE: {study_lgbm.best_value}\")\n",
    "print(f\"Internal Test RMSE: {actual_test_rmse}\")\n",
    "print(f\"Best LGBM params: {study_lgbm.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna code for CatBoostRegressor\n",
    "\n",
    "\n",
    "def objective_cat(trial):\n",
    "    param = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'random_seed': 45, # CatBoost يستخدم اسم random_seed\n",
    "        'verbose': False,\n",
    "        'iterations': trial.suggest_int('iterations', 2000, 4000),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(**param)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=45)\n",
    "    \n",
    "    score = cross_val_score(model, X_train_preprocessed, y_train, \n",
    "                            cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    return -score.mean()\n",
    "\n",
    "sampler_cat = optuna.samplers.TPESampler(seed=45)\n",
    "study_cat = optuna.create_study(direction='minimize', sampler=sampler_cat)\n",
    "study_cat.optimize(objective_cat, n_trials=50)\n",
    "\n",
    "final_cat = CatBoostRegressor(**study_cat.best_params, random_seed=400, verbose=False)\n",
    "final_cat.fit(X_train_preprocessed, y_train, verbose=False)\n",
    "\n",
    "y_pred_cat = final_cat.predict(X_test_preprocessed)\n",
    "actual_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_cat))\n",
    "\n",
    "print(f\"Best CatBoost RMSE: {study_cat.best_value}\")\n",
    "print(f\"Internal Test RMSE: {actual_test_rmse}\")\n",
    "print(f\"Best CatBoost params: {study_cat.best_params}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
