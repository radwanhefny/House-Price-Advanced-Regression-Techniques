# ğŸ  Ames Housing Price Prediction | Advanced Regression Techniques

This project implements an **end-to-end machine learning pipeline** for predicting house prices on the Ames Housing dataset. It combines manual ordinal encoding, target encoding, robust preprocessing, and a weighted ensemble of Ridge, CatBoost, XGBoost, LightGBM, and Gradient Boosting Regressor, with **hyperparameter tuning using Optuna**, optimized for performance and generalization.

Kaggle Public Score: **0.12260 | Top 12% (Rank 586 / 4764)**

---

## âœ¨ Features
- Robust **data cleaning** and **EDA-driven outlier removal**.
- Advanced **feature engineering**:
  - Manual ordinal encoding
  - Target encoding
  - New features: house age, total area, total baths, total porch area
- **Pipeline-based preprocessing** for numerical and categorical features.
- Custom **weighted ensemble** combining multiple regressors.
- **Hyperparameter tuning** for all models using **Optuna**.
- Generates Kaggle-ready submission automatically.

---

## ğŸ“‹ Prerequisites
- Python 3.8+
- Libraries: `numpy`, `pandas`, `scikit-learn`, `xgboost`, `catboost`, `lightgbm`, `category_encoders`
- Dataset: Place `train.csv` and `test.csv` inside a `data/` folder

---

## ğŸš€ Getting Started
1. Clone the repository:
```bash
git clone https://github.com/radwanhefny/House-Price-Advanced-Regression-Techniques.git
cd House-Price-Advanced-Regression-Techniques
```
2. Install dependencies:
```bash
pip install -r requirements.txt
```
3. Run the pipeline script:
```bash
python house-prices-advanced-regression-techniques.py
```
This will preprocess the data, train the ensemble, and generate submission.csv.

---

## ğŸ¬ Screenshots / Demo

### ğŸ“Š Scatter Plot: outlier detection 
Shows relationship between features with outliers and target variable.

<img src="https://github.com/radwanhefny/House-Price-Advanced-Regression-Techniques/blob/main/pictures/scatter.png" width="900"/>

### ğŸ¯ Target Transformation 
Before and after log transformation of SalePrice.

| Before | After |
|--------|-------|
| <img src="https://github.com/radwanhefny/House-Price-Advanced-Regression-Techniques/blob/main/pictures/target1.png" width="450"/> | <img src="https://github.com/radwanhefny/House-Price-Advanced-Regression-Techniques/blob/main/pictures/target2.png" width="450"/> |


### ğŸ”¥ Feature Correlation Heatmap  
After dropping highly correlated features (+0.80).

<img src="https://github.com/radwanhefny/House-Price-Advanced-Regression-Techniques/blob/main/pictures/heatmap.png" width="900"/>

---

## ğŸ—‚ï¸ Project Structure
```
ğŸ“ House-Price-Advanced-Regression-Techniques
â”œâ”€â”€ house-prices-advanced-regression-techniques.py   # Full ML pipeline
â”œâ”€â”€ house-prices-advanced-regression-techniques.ipynb   # Full ML jupyter notebook
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â””â”€â”€ sample_submission.csv
â”œâ”€â”€ pictures/                                        # Visualizations
â”‚   â”œâ”€â”€ scatter.png
â”‚   â”œâ”€â”€ target1.png
â”‚   â”œâ”€â”€ target2.png
â”‚   â””â”€â”€ heatmap.png
â”œâ”€â”€ submission.csv                                # Kaggle submission
â”œâ”€â”€ hyperparameter_optimization.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ› ï¸ Usage
- Run the pipeline script to generate preprocessed data, train models with tuned hyperparameters, and produce Kaggle-ready predictions.
- Outputs:
  - submission.csv â†’ Kaggle submission.
  - Internal validation RMSE printed in console.
- Expected Kaggle Public Score: 0.12260 | Top 12%.

---

## âœ… Evaluation Metrics
- Root Mean Squared Error (RMSE)
- Cross-validation RMSE
- Kaggle Public Score

---

## ğŸ§  How It Works
1. Load datasets using Pandas.
2. Clean missing values and outliers.
3. Apply feature engineering and encoding:
    - Ordinal encoding
    - Target encoding
    - Log transformation of target
    - New engineered features
4. Split train/validation sets.
5. Build preprocessing pipeline (numerical scaling, categorical encoding).
6. Train ensemble of Ridge, CatBoost, XGBoost, LightGBM, Gradient Boosting.
7. Tune hyperparameters using Optuna for stability and performance.
8. Generate Kaggle-ready predictions.

### ğŸ§ª Experimental Notes
- Tested Random Forest: underperformed compared to ensemble.  
- Tried manual ensemble: similar performance to Voting, but Voting automated the process.  
- Explored stacking in multiple versions: Voting consistently gave better results, so it was chosen.  
- Added Linear Regression in the final version: overfitting occurred, reducing ensemble performance due to sensitivity to extreme values.

---

## ğŸ¤ Contributing
Contributions are welcome!
1. Fork the repository
2. Create a new feature branch
3. Submit a pull request
Please ensure your code is clean, structured, and well-commented.


---


## ğŸ“ License
This project is licensed under the MIT license - see the LICENSE file for details. 


---


## ğŸ“ Support
If you have questions or need help, feel free to:
- Open an issue on this repository  
- Connect with me on LinkedIn: https://www.linkedin.com/in/radwanhefny  
